{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import python module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For coding Random Forest, we have to create two classes, decision tree and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating decision tree\n",
    "class DecisionTree():\n",
    "    '''\n",
    "    x: independant variables of training set\n",
    "    y: dependent variables of training set\n",
    "    n_feature: number of features in sample\n",
    "    f_idxs: \n",
    "    idxs: stores the indices of the rows this tree contains\n",
    "    sample_size: size of sample\n",
    "    depth: depth of each decision tree\n",
    "    min_leaf: minimum number of rows in a node\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, x, y, n_features, f_idxs, idxs, depth=7, min_leaf=4):\n",
    "        '''\n",
    "        decision tree constructor\n",
    "        '''\n",
    "        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        '''\n",
    "        This function loops through all the columns sequentially and finds the best split among them all.\n",
    "        It makes only single split.\n",
    "        '''\n",
    "        for i in self.f_idxs: \n",
    "            self.find_better_split(i)\n",
    "       \n",
    "\n",
    "    def find_better_split(self, var_idx):\n",
    "        '''\n",
    "        It finds the best possible split in a certain column.\n",
    "        '''\n",
    "        x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "    \n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "                \n",
    "#We will now define some property decorator to make our code more concise.\n",
    "    @property\n",
    "    def split_name(self):\n",
    "        '''\n",
    "        It will return the name of the column weâ€™re splitting over.\n",
    "        '''\n",
    "        return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        '''\n",
    "        It will segregate a column with selected rows.\n",
    "        '''\n",
    "        return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        It will identify leaf nodes.\n",
    "        '''\n",
    "        return self.score == float('inf') or self.depth <= 0 \n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        makes prediction for our decision tree\n",
    "        '''\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        '''\n",
    "        Predicts the rows for decision tree.\n",
    "        '''\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating random forest class\n",
    "class RandomForest():\n",
    "    '''\n",
    "    x: independant variables of training set\n",
    "    y: dependent variables of training set\n",
    "    n_trees: number of uncorrelated trees\n",
    "    n_feature: number of features in sample\n",
    "    sample_size: size of sample\n",
    "    depth: depth of each decision tree\n",
    "    min_leaf: minimum number of rows in a node\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, x, y, n_trees, n_features, sample_size, depth=7, min_leaf=4):\n",
    "        '''\n",
    "        In this constructor, we defined our random forest regressor.\n",
    "        '''\n",
    "        np.random.seed(10)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        '''\n",
    "        This constructor creates a new decision tree by calling Decision Tree.\n",
    "        '''\n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                    idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Here we predict our output \n",
    "        '''\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "\n",
    "def std_agg(cnt, s1, s2):\n",
    "    '''\n",
    "    Predicts the accuracy of our Random Forest Classifier\n",
    "    '''\n",
    "    return math.sqrt((s2/cnt) - (s1/cnt)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
