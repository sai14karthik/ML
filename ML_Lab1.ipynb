{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab Assignemnt 1\n",
    "- Sai Karthik \n",
    "AP21110010310\n",
    "CSE-E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Implement Linear Regression and calculate sum of residual error on the following\n",
    "Datasets.\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "y = [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]\n",
    "#\n",
    "i) Compute the regression coefficients using analytic formulation and calculate Sum\n",
    "Squared Error (SSE) and R2 value.\n",
    "#\n",
    "ii) Implement gradient descent (both Full-batch and Stochastic with stopping\n",
    "criteria) on Least Mean Square loss formulation to compute the coefficients of\n",
    "regression matrix and compare the results using performance measures such as R2\n",
    "SSE etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x, y):\n",
    "    n = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "    intercept = y_mean - slope * x_mean\n",
    "    \n",
    "    y_pred = slope * x + intercept\n",
    "    \n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r2 = 1 - (sse / ss_total)\n",
    "    \n",
    "    return slope, intercept, sse, r2\n",
    "\n",
    "slope, intercept, sse, r2 = linear_regression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 1.1696969696969697\n",
      "Intercept: 1.2363636363636363\n",
      "Sum of Squared Error (SSE): 5.624242424242423\n",
      "R^2 value: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Sum of Squared Error (SSE):\", sse)\n",
    "print(\"R^2 value:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(x, y, learning_rate=0.01, epochs=1000, batch_size=None):\n",
    "    n = len(x)\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    \n",
    "    if batch_size is None:\n",
    "        batch_size = n\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if batch_size == n:\n",
    "            x_batch = x\n",
    "            y_batch = y\n",
    "        else:\n",
    "            indices = np.random.choice(n, batch_size)\n",
    "            x_batch = x[indices]\n",
    "            y_batch = y[indices]\n",
    "        \n",
    "        y_pred = slope * x_batch + intercept\n",
    "        \n",
    "        slope_gradient = -(2 / batch_size) * np.sum(x_batch * (y_batch - y_pred))\n",
    "        intercept_gradient = -(2 / batch_size) * np.sum(y_batch - y_pred)\n",
    "        \n",
    "        slope -= learning_rate * slope_gradient\n",
    "        intercept -= learning_rate * intercept_gradient\n",
    "        \n",
    "    y_pred = slope * x + intercept\n",
    "    \n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r2 = 1 - (sse / ss_total)\n",
    "    \n",
    "    return slope, intercept, sse, r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full-batch gradient descent:\n",
      "Slope: 1.170263693076768\n",
      "Intercept: 1.2328099487610318\n",
      "Sum of Squared Error (SSE): 5.624278989977716\n",
      "R2 value: 0.9525377300423822\n",
      "\n",
      "Stochastic gradient descent:\n",
      "Slope: 1.1684728386473255\n",
      "Intercept: 1.275337914338942\n",
      "Sum of Squared Error (SSE): 5.63556557313237\n",
      "R2 value: 0.9524424846149168\n"
     ]
    }
   ],
   "source": [
    "slope_batch, intercept_batch, sse_batch, r2_batch = linear_regression_gradient_descent(x, y)\n",
    "print(\"\\nFull-batch gradient descent:\")\n",
    "print(\"Slope:\", slope_batch)\n",
    "print(\"Intercept:\", intercept_batch)\n",
    "print(\"Sum of Squared Error (SSE):\", sse_batch)\n",
    "print(\"R2 value:\", r2_batch)\n",
    "\n",
    "slope_stochastic, intercept_stochastic, sse_stochastic, r2_stochastic = linear_regression_gradient_descent(x, y, batch_size=2)\n",
    "print(\"\\nStochastic gradient descent:\")\n",
    "print(\"Slope:\", slope_stochastic)\n",
    "print(\"Intercept:\", intercept_stochastic)\n",
    "print(\"Sum of Squared Error (SSE):\", sse_stochastic)\n",
    "print(\"R2 value:\", r2_stochastic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download Boston Housing Rate Dataset. Analyse the input attributes and find out the\n",
    "attribute that best follow the linear relationship with the output price. Implement both the\n",
    "analytic formulation and gradient descent (Full-batch, stochastic) on LMS loss\n",
    "formulation to compute the coefficients of regression matrix and compare the results.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of input attributes with the output price:\n",
      "PRICE      1.000000\n",
      "LSTAT      0.737663\n",
      "RM         0.695360\n",
      "PTRATIO    0.507787\n",
      "INDUS      0.483725\n",
      "TAX        0.468536\n",
      "NOX        0.427321\n",
      "CRIM       0.388305\n",
      "RAD        0.381626\n",
      "AGE        0.376955\n",
      "ZN         0.360445\n",
      "B          0.333461\n",
      "DIS        0.249929\n",
      "CHAS       0.175260\n",
      "Name: PRICE, dtype: float64\n",
      "R^2 scores:\n",
      "Analytic solution: 0.6687594935356311\n",
      "Full-batch gradient descent: 0.6514150559454255\n",
      "Stochastic gradient descent: 0.5589752905161083\n",
      "\n",
      "MSE scores:\n",
      "Analytic solution: 24.291119474973584\n",
      "Full-batch gradient descent: 25.563052700251475\n",
      "Stochastic gradient descent: 32.34201041363614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = fetch_openml(name='boston', version=1)\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "correlations = np.abs(np.corrcoef(X.T, y)[:,-1])[:-1]\n",
    "\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['PRICE'] = boston.target\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "correlation_with_price = correlation_matrix['PRICE'].abs().sort_values(ascending=False)\n",
    "print(\"Correlation of input attributes with the output price:\")\n",
    "print(correlation_with_price)\n",
    "X_train_with_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n",
    "theta_analytic = np.linalg.inv(X_train_with_bias.T.dot(X_train_with_bias)).dot(X_train_with_bias.T).dot(y_train)\n",
    "print()\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    for i in range(num_iters):\n",
    "        gradient = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta -= alpha * gradient\n",
    "    return theta\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "theta_full_batch = np.random.randn(X_train_with_bias.shape[1])\n",
    "theta_full_batch = gradient_descent(X_train_with_bias, y_train, theta_full_batch, alpha, num_iters)\n",
    "\n",
    "def stochastic_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    for i in range(num_iters):\n",
    "        for j in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            Xj = X[random_index:random_index+1]\n",
    "            yj = y[random_index:random_index+1]\n",
    "            gradient = Xj.T.dot(Xj.dot(theta) - yj)\n",
    "            theta -= alpha * gradient\n",
    "    return theta\n",
    "\n",
    "theta_stochastic = np.random.randn(X_train_with_bias.shape[1])\n",
    "theta_stochastic = stochastic_gradient_descent(X_train_with_bias, y_train, theta_stochastic, alpha, num_iters)\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    mean_y = np.mean(y_true)\n",
    "    total_sum_squares = np.sum((y_true - mean_y) ** 2)\n",
    "    residual_sum_squares = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (residual_sum_squares / total_sum_squares)\n",
    "    return r2\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "X_test_with_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "y_pred_analytic = X_test_with_bias.dot(theta_analytic)\n",
    "y_pred_full_batch = X_test_with_bias.dot(theta_full_batch)\n",
    "y_pred_stochastic = X_test_with_bias.dot(theta_stochastic)\n",
    "\n",
    "r2_analytic = calculate_r2(y_test, y_pred_analytic)\n",
    "r2_full_batch = calculate_r2(y_test, y_pred_full_batch)\n",
    "r2_stochastic = calculate_r2(y_test, y_pred_stochastic)\n",
    "\n",
    "mse_analytic = calculate_mse(y_test, y_pred_analytic)\n",
    "mse_full_batch = calculate_mse(y_test, y_pred_full_batch)\n",
    "mse_stochastic = calculate_mse(y_test, y_pred_stochastic)\n",
    "print()\n",
    "print(\"R^2 scores:\")\n",
    "print(\"Analytic solution:\", r2_analytic)\n",
    "print(\"Full-batch gradient descent:\", r2_full_batch)\n",
    "print(\"Stochastic gradient descent:\", r2_stochastic)\n",
    "print()\n",
    "print(\"\\nMSE scores:\")\n",
    "print(\"Analytic solution:\", mse_analytic)\n",
    "print(\"Full-batch gradient descent:\", mse_full_batch)\n",
    "print(\"Stochastic gradient descent:\", mse_stochastic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
